% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/V-training.R
\name{train_magmaclust}
\alias{train_magmaclust}
\title{Training MagmaClust with a Variational EM algorithm}
\usage{
train_magmaclust(
  data,
  nb_cluster = NULL,
  prior_mean_k = NULL,
  ini_hp_k = NULL,
  ini_hp_i = NULL,
  kern_k = "SE",
  kern_i = "SE",
  ini_mixture = NULL,
  common_hp_k = T,
  common_hp_i = T,
  n_iter_max = 25,
  pen_diag = 1e-08,
  cv_threshold = 0.001
)
}
\arguments{
\item{data}{A tibble or data frame. Columns required: \code{ID}, \code{Input}
, \code{Output}. Additional columns for covariates can be specified.
The \code{ID} column contains the unique names/codes used to identify each
individual/task (or batch of data).
The \code{Input} column should define the variable that is used as
reference for the observations (e.g. time for longitudinal data). The
\code{Output} column specifies the observed values (the response
variable). The data frame can also provide as many covariates as desired,
with no constraints on the column names. These covariates are additional
inputs (explanatory variables) of the models that are also observed at
each reference \code{Input}.}

\item{nb_cluster}{A number, indicating the number of clusters/groups of
individuals/tasks that are assumed to exist among the dataset.}

\item{prior_mean_k}{The set of hyper-prior mean parameters (m_k) of the mean
GPs, one value for each cluster.
cluster. This argument can be specified under various formats, such as:
\itemize{
\item NULL (default). All hyper-prior means would be set to 0 everywhere.
\item A vector of the same length as the number of clusters. Each hyper-prior
mean would be a constant function equal to one element of the vector.
\item A vector of the same length as all the distinct Input values in the
\code{data} argument. This vector would be considered as the evaluation
of all hyper-prior mean functions at the training Inputs.
\item A function. This function is defined as the hyper_prior means.
\item A tibble or data frame. Required columns: Input, Output. The Input
values should include at least the same values as in the \code{data}
argument.
}}

\item{ini_hp_k}{A tibble or data frame of hyper-parameters
associated with \code{kern_k}, the mean process' kernel. The
columns/elements should be named according to the hyper-parameters
that are used in \code{kern_k}.}

\item{ini_hp_i}{A tibble or data frame of hyper-parameters
associated with \code{kern_i}, the individual processes' kernel.
Required column : \code{ID}. The \code{ID} column contains the unique
names/codes used to identify each individual/task. The other columns
should be named according to the hyper-parameters that are used in
\code{kern_i}.}

\item{kern_k}{A kernel function, associated with the mean GPs.
Several popular kernels
(see \href{https://www.cs.toronto.edu/~duvenaud/cookbook/}{The Kernel
Cookbook}) are already implemented and can be selected within the
following list:
\itemize{
\item "SE": (default value) the Squared Exponential Kernel (also called
Radial Basis Function or Gaussian kernel),
\item "LIN": the Linear kernel,
\item "PERIO": the Periodic kernel,
\item "RQ": the Rational Quadratic kernel.
Compound kernels can be created as sums or products of the above kernels.
For combining kernels, simply provide a formula as a character string
where elements are separated by whitespaces (e.g. "SE + PERIO"). As the
elements are treated sequentially from the left to the right, the product
operator '*' shall always be used before the '+' operators (e.g.
'SE * LIN + RQ' is valid whereas 'RQ + SE * LIN' is  not).
}}

\item{kern_i}{A kernel function, associated with the individual GPs. ("SE",
"PERIO" and "RQ" are also available here)}

\item{ini_mixture}{Initial values of probability to belong to each cluster
for each individual.}

\item{common_hp_k}{A boolean indicating whether hp are common among mean GPs
(for each \eqn{mu_k}).}

\item{common_hp_i}{A boolean indicating whether hp are common among
individual GPs (for each \eqn{y_i}).}

\item{n_iter_max}{A number, indicating the maximum number of iterations of
the EM algorithm to proceed while not reaching convergence.}

\item{pen_diag}{A number. A jitter term, added on the diagonal to prevent
numerical issues when inverting nearly singular matrices.}

\item{cv_threshold}{A number, indicating the threshold of the likelihood gain
under which the VEM algorithm will stop. The convergence condition is
defined as the difference of elbo between two consecutive steps,
divided by the absolute value of the last one
( \eqn{(ELBO_n - ELBO_{n-1}) / \abs{ELBO_n}} ).}
}
\value{
A list, containing the results of a variation of the EM algorithm
used for training in MagmaClust. The elements of the list are:
\itemize{
\item hp_k: A tibble containing the trained hyper-parameters for the mean
process' kernel.
\item hp_i: A tibble containing all the trained hyper-parameters for the
individual processes' kernels.
\item prop_mixture_k : A tibble containing the hyper-parameters associated
with each individual, indicating in which cluster it belongs.
\item param: A list 3 containing  The mean, the cov and the mixture.
-> mean: A tibble containing the values of hyper-posterior's mean
parameter (\code{Output}) evaluated at each training reference
-> cov: A matrix, covariance parameter of the hyper-posterior
distribution of the mean process.
-> mixture: probability to belong to a cluster for an individual.
\code{Input}.
\item ini_args: A list containing the initial values for the hyper-prior mean,
the hyper-parameters, and the kernels that have been defined and used
during the learning procedure.
\item Convergence: A logical value indicated whether the algorithm converged.
\item Training_time: Total running time of the complete training.
}
}
\description{
The hyper-parameters and the hyper-posterior distribution involved in
MagmaClust can be learned thanks to a VEM algorithm implemented in
\code{train_magmaclust}. By providing a dataset, the model hypotheses
(hyper-prior mean parameters and covariance kernels) and initialisation
values for the hyper-parameters, the function computes maximum likelihood
estimates of the HPs as well as the mean and covariance parameters of the
Gaussian hyper-posterior distributions of the mean processes.
}
\examples{
\dontrun{
k = seq_len(3)
m_k <- c("K1" = 0, "K2" = 0, "K3" = 0)

db <- simu_db(N = 10, common_input = TRUE)
hp_k <- MagmaClustR:::hp("SE", list_ID = names(m_k))
hp_i <- MagmaClustR:::hp("SE", list_ID = unique(db$ID))
old_mixture = MagmaClustR:::ini_mixture(db = db, k = length(k),
 nstart = 50)

train_magmaclust(db, length(k), m_k, hp_k, hp_i,
"SE", "SE", old_mixture, FALSE, FALSE, 25, 0.1, 1e-3)

###############################

db <- simu_db()
train_magmaclust(db)
}
}
