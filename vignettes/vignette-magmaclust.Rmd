---
title: Introduction to MagmaClust
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to MagmaClust}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: sentence
---

<style>
body {
text-align: justify}
</style>

```{r, echo = FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
options(tibble.print_min = 6L, tibble.print_max = 6L)
knitr::opts_chunk$set(fig.align="center")
library(MagmaClustR)
library(dplyr)
library(readr)
children = read_csv("data/db_gusto_weight.csv")
children$ID <- factor(as.numeric(factor(children$ID)))

set.seed(5)
```

First, we load the libraries we use to handle MagmaClust and our data.

```{r, eval=FALSE}
library(MagmaClustR)
library(dplyr)
library(ggplot2)
```


## Data and purpose

### Context

To explore the features of MagmaClust, we use the `children` dataset available [here](https://github.com/ArthurLeroy/MAGMAclust/blob/master/Real_Data_Study/Data/db_gusto_weight.csv). This dataset contains `r length(unique(children %>% filter(sex=="Male") %>% pull(ID)))` boys and `r length(unique(children %>% filter(sex=="Female") %>% pull(ID)))` girls, for a total of `r nrow(children)` children weights.

Throughout this `children` example, we use MagmaClust to model children’s evolution curves and forecast their future weight. 
More specifically, we extend the Magma model to take into account the possible presence of group structures in the data (Magma’s Vignette available here [METTRE LIEN](lien)).  We thus introduce a clustering component into the procedure, which is based on gaussian mixtures.  
Indeed, the combination of a GPs mixture model with a multi-task aspect offers enhanced predictive abilities by sharing information across the individuals through multiple cluster-specific mean processes.

More generally, our task is to train a model with a large dataset and predict a new individual’s weight thanks to shared information within a cluster.

To get an idea of how our `children` dataset looks, we display the weight of 5 children according to their age. Do the children’s weight evolve the same, *i.e.* with the same pattern ?  Can we identify group structures in the data, and thus define weight evolution clusters ?


```{r fig.asp = 0.6, fig.width = 7, out.width = "80%"}
ggplot2::ggplot(data=children %>% filter(ID %in% 1:5),
       mapping = ggplot2::aes(x=Input,y=Output,colour=ID))+
  ggplot2::geom_point(size=3) +
  ggplot2::theme_classic() +
  ggplot2::guides(colour="none")
```

This plot underlines the regularity of our functional data; children’s weight are observed at the same ages, probably because of regular visits to the pediatrician. However, MagmaClust can also be used in case of irregular grids of `Input` without any change thanks to its shared information feature.

Overall, it clearly appears that there are different children profiles; some children are plump at birth but their weight gain slows down quickly, while others are born thinner but grow rapidly. Therefore, it might be important to take these profiles into account if one wants to improve the quality of weight predictions.

### Dataset format

The `children` dataset contains 4 columns: `ID`, `sex`, `Input` and `Output`.
In our case, each row represents a child’s weight at a given age.

*   The `ID` column is used to identify children (who are anonymized) and study their evolution over time. It must be a character or a factor;

*   The `sex` column is a factor. It can take the values `Male` and `Female`;

*   The `Input` (here, `Age` in months) column corresponds to the child's age  and must be numeric;

*   The `Output` (here, `Weight`) column corresponds to the child’s weight and must be numeric.




```{r}
knitr::kable(children[1:5,])
```

Any additional column (such as height, morphology, percentage of muscle mass ...) would be treated as a covariate, and thus define multi-dimensional inputs.

Before starting to use MagmaClust, we must ensure that :

*   Our dataset contains the columns `ID`, `Input` and `Output`;

*   Their types are as previously introduced.

Therefore, we need to remove the `sex` column before using MagmaClust. 

```{r}
children <- children %>% select(-sex)
```

Moreover, as the children weight is not very different according to the gender between 0 and 5 years old, we do not split males and females into separate datasets. 


## Classical pipeline

The overall process with MagmaClust can be decomposed in 3 main steps: **training, prediction** and **display of results**.
The corresponding functions are:

-   [`train_magmaclust()`](https://arthurleroy.github.io/MagmaClustR/reference/train_magmaclust.html)
-   [`pred_magmaclust()`](https://arthurleroy.github.io/MagmaClustR/reference/pred_magmaclust.html)
-   [`plot_magmaclust()`](https://arthurleroy.github.io/MagmaClustR/reference/plot_magmaclust.html)

## Let's use MagmaClust on our children

### Data organisation


Once the data organisation step is done, we randomly select two types of children :

*   those we use to train the model;

*   the one for whom we predict future weights; let's give him the fictive name of James.

Since our dataset is quite large, we randomly select only 16 children for the training step. Even if MagmaClust performances improve with respect to the number of training children, 16 are enough to get a clear idea of how the algorithm works.


```{r}
set.seed(10)
list_ID <- children %>% pull(ID) %>% sample()
children_train <- children %>% filter(ID %in% list_ID[1:20])
children_pred <- children %>% filter(ID == 72) %>% filter(Input<20)
```


```{r fig.asp = 0.65, fig.width = 7, out.width = "80%"}
ggplot2::ggplot(data = children_train,
       mapping = ggplot2::aes(x=Input,y=Output,colour=ID))+
  ggplot2::geom_point(size=1.5,alpha=0.3)+
  ggplot2::geom_point(data = children_pred,
             size=3,
             shape=17,
             col="black") +
  ggplot2::theme_classic() +
  ggplot2::guides(colour="none")
```

The triangles correspond to James’ weights, whereas the colored dots are the children' data we use for training.

### Training


Once we split the dataset into `train` and `pred`, we can proceed to the training of the model with the `train_magmaclust()` function. Now that we have a good idea of how our data looks, we can specify values for several parameters:

*  `nb_cluster`:  as for any clustering method, we have to provide a number *K* of clusters as an hypothesis of the model. Since we do not have a clear idea of the real number of clusters in our subset, we use the `select_nb_clusters()` function; it is a model selection method based on maximising a VBIC criterion that returns the true number of clusters (*K*) within a dataset. Here, we get `nb_cluster = 3`.

```{r, eval=FALSE}
selection <- select_nb_cluster(data = children,
                  grid_nb_cluster = 1:5,
                  kern_k = "SE",
                  kern_i = "SE")

best_nb_clust <- selection$best_k
```



*   `kernel`: Relationship between observed data and prediction targets are customizable in the covariance **kernels**. Therefore, if we want to have a curve that fits our data, we have to choose suitable covariance kernels. In the case of children, we can set `kern_k = SE` and `kern_i = SE` to bring smoothness to the evolution curves.

The most commonly used kernels and their properties are shown in [the kernel cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/).
Details of available kernels and how to combine them are available in [`help train_magma`](https://arthurleroy.github.io/MagmaClustR/reference/train_magma.html). 


*   `common_hp_k`: here, we assume that the set of hyper-parameters is the same for each cluster by setting `common_hp_k = TRUE`. Thus, we suppose that all clusters share the same covariance structure. This property implies that the shapes and variations of the curves are assumed to be roughly identical from one cluster to another; the differentiation is mainly due to the mean values.

*  `common_hp_i`: as we want to share information across children, we specify `common_hp_i = TRUE`. If this assumption may appear restrictive at first glance, it actually offers a valuable way to share common patterns between tasks.

As for any GP method, initialisation of the HP may have a huge influence on the final optimisation  and leads to inadequate prediction for pathological cases.

Other parameters can also be modified; see [`help train_magmaclust`](https://arthurleroy.github.io/MagmaClustR/reference/train_magmaclust.html) for more details.



```{r}
model_clust <- train_magmaclust(data = children_train,
                        nb_cluster = 3,
                        kern_k = "SE",
                        kern_i = "SE",
                        common_hp_k = FALSE,
                        common_hp_i = FALSE)
```


### James' prediction

As the MagmaClust model is trained, we can predict the evolution of James' weight. To perform prediction, we need to specify two main parameters in the `pred_magmaclust()` function:

*   `data`: in our case, data associated to James;
*   `trained_model`, which corresponds to the model we just trained with the other 15 children.

We recall that we want to study weight curves during childhood. Therefore, we should predict the evolution of James’ weight between 0 and 6 years (72 months), which leads to set `grid_inputs = seq(0,72,0.1)`.



```{r}
pred_clust <- pred_magmaclust(data = children_pred,
  trained_model = model_clust,
  grid_inputs = seq(0,72,0.1),
  plot = FALSE,
  get_hyperpost = TRUE)
```

Here we specified `get_hyperpost = TRUE` to plot the average GP of each cluster in the `plot_magmaclust()` function.
However, we do not need it if we only want the prediction of our individual.


### Plots


With the [`plot_magmaclust()`](https://arthurleroy.github.io/MagmaClustR/reference/plot_magmaclust.html) function, we can display the evolution of James' weight.  In particular, we show:

*   James' evolution curve in purple;

*   the training data points in the background. Each color corresponds to one individual;

*   the mean GP of each cluster in dotted lines thanks to the `prior_mean` parameter.


#### MagmaClust VS Magma

The MagmaClustR package also provides a prediction with the Magma model; thus, we can compare MagmaClust and Magma predictions. For more details on Magma, see [Vignette Magma](Lien). 

The graphs below correspond to James' weight prediction with MagmaClust (left) and Magma (right). 


```{r, echo = FALSE, message = FALSE, include = FALSE}
model_magma <- train_magma(data = children_train,
                        kern_0 = "SE",
                        kern_i = "SE",
                        common_hp = FALSE)

pred_magma <- pred_magma(data=children_pred,
                   grid_inputs = seq(0,72,0.1),
                   trained_model = model_magma,
                   plot = FALSE,
                   get_hyperpost = TRUE) 

gg2 <- plot_gp(pred_gp = pred_magma,
        data = children_pred,
        prior_mean = pred_magma$hyperpost$mean,
        data_train = children_train,
        size_data = 5) + 
  ggplot2::scale_y_continuous(limits = c(0,37))

data_train_with_clust = data_allocate_cluster(model_clust)

gg1 <- plot_magmaclust(pred = pred_clust,
                data = children_pred,
                data_train = children_train,
                prior_mean = pred_clust$hyperpost$mean,
                size_data = 5) + 
  ggplot2::scale_y_continuous(limits = c(0,37))
```


```{r, eval=FALSE}
plot_magmaclust(pred = pred_clust,
                data = children_pred,
                prior_mean = pred_clust$hyperpost$mean,
                data_train = children_train,
                size_data = 5)
```



```{r fig.asp = 0.7, fig.width = 10, out.width = "100%", echo = FALSE,warning=FALSE}
gg1
gg2
```


On intervals of unobserved timestamps containing data points from the training dataset ($t \in ]20, 72]$), Magma takes advantage of its multi-task component to share knowledge across individuals by estimating a relevant mean process.
However, this unique mean process appears unable to account for the group structures among children, although adequately recovering the dispersion of their weight.
Thus, MagmaClust brings a significant improvement in mean prediction thanks to its gaussian mixtures feature (through the multiple mean processes)

However, difference of prediction performances between Magma and MagmaClust depends highly on the structure of our data (*is it fuzzy or well-seperated?*), as well as on the initialisation.

We can notice oscillations in the average GPs of the clusters, which comes from several things:

  * The data itself contains oscillations;
  * We took `SE` kernels, which favor oscillations;
  * Data is observed only at particular time points.
  

## Even prettier graphics

We can distinguish 2 cases:

*   Michaela belongs to one of the clusters with a probability of 1. We can then display the associated 95% credibility interval;

*   Michaela belongs to several clusters; therefore, its evolution curve is a mixture of gaussians.
In this case, there is no associated credibility interval. However, a heatmap of probabilities is available if we want to estimate the "most likely" areas for the curve.

We can also display training individuals in the same color as their cluster mean GP by using the [`data_allocate_cluster()`](https://arthurleroy.github.io/MagmaClustR/reference/data_allocate_cluster.html) function. We also specify the arguments `data_train = data_train_with_clust` and `col_clust = TRUE`.


```{r fig.asp = 0.65, fig.width = 7, out.width = "80%"}
data_train_with_clust = data_allocate_cluster(model_clust)

plot_magmaclust(pred = pred_clust,
                cluster = "all",
                data = children_pred,
                prior_mean = pred_clust$hyperpost$mean,
                heatmap = TRUE,
                y_grid = seq(0,37,0.1),
                data_train = data_train_with_clust,
                col_clust = TRUE,
                size_data = 5)
```

## Generalisation

Overall, the multi-task framework combined with clustering method provides reliable probabilistic predictions for a new child on a wide range of timestamps.
Furthermore, the uncertainty provided through the predictive posterior distribution offers an adequate degree of caution for pediatricians as well as parents. 

However, these satisfying results do not come from James’ particular example. For further details, the complete study and experiments are available; see [Cluster-Specific Predictions with Multi-Task Gaussian Processes](https://arxiv.org/pdf/2011.07866.pdf).




