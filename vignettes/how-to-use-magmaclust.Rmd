---
title: Introduction to MagmaClust
output: 
   prettydoc::html_pretty:
    theme: leonids
    highlight: github
vignette: >
  %\VignetteIndexEntry{Introduction to MagmaClust}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: sentence
---

<style>
body {
text-align: justify}
</style>

```{r, echo = FALSE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(dpi=300, collapse = T, comment = "#>")
options(tibble.print_min = 6L, tibble.print_max = 6L)
knitr::opts_chunk$set(fig.align="center")
```


```{r echo = FALSE, message = FALSE}
library(MagmaClustR)
library(dplyr)
library(ggplot2)
```


## Data and purpose

### Context

To explore the features of MagmaClust, we use the `weight` dataset available [here](https://github.com/ArthurLeroy/MAGMAclust/blob/master/Real_Data_Study/Data/db_gusto_weight.csv). This dataset contains `r length(unique(weight %>% filter(sex=="Male") %>% pull(ID)))` boys and `r length(unique(weight %>% filter(sex=="Female") %>% pull(ID)))` girls, for a total of `r nrow(weight)` children weights.

Throughout this `weight` follow-up example, we use *MagmaClust* to model and forecast evolution of children’s weight. 
More specifically, we extend the *Magma* model to take into account the possible presence of group structures in the data (a similar vignette for Magma is available [here](how-to-use-magma.html)). Therefore, a clustering component based on gaussian mixtures extend the procedure.  
The combination of a GPs mixture model with a multi-task aspect offers enhanced predictive abilities by sharing information across the individuals through multiple cluster-specific mean processes.

More generally, our aim is to train a model on a dataset containing multiple individuals and predict a new individual’s weight thanks to shared information within clusters.

To get an idea of how our `weight` dataset looks, we display an illustration of observed data for 5 children according to their age. Do the children’s weight evolve the same, *i.e.* with the same pattern?  Can we identify group structures in the data, and thus define appropriate clusters?


```{r fig.asp = 0.6, fig.width = 7, out.width = "80%"}
set.seed(10)
list_ID <- weight %>% pull(ID) %>% sample()

ggplot2::ggplot(data=weight %>% filter(ID %in% list_ID[1:5]),
                ggplot2::aes(x=Input,y=Output,colour=factor(ID)))+
  ggplot2::geom_point(size=3) +
  ggplot2::theme_classic() +
  ggplot2::guides(colour="none")
```

This plot underlines the regularity of our functional data; he values of weight are observed at the same ages, probably because of regular visits to the pediatrician. Let us note that MagmaClust could also be used in the case of irregular grids of `Input` without any change.

Overall, it clearly appears that there are different children profiles; some children are plump at birth before their weight gain slows down quickly, while others may born slightly thinner but grow faster. Therefore, it might be important to take these profiles into account if one wants to improve the quality of weight predictions.

### Dataset format

The `weight` dataset contains 4 columns: `ID`, `sex`, `Input` and `Output`.
In our case, each row represents a child’s weight at a given age.

*   The `ID` column is used to identify children (who are anonymized) and study their evolution over time. It must be a character or a factor;

*   The `sex` column is a factor. It can take the values `Male` and `Female`;

*   The `Input` (here, `Age` in months) column corresponds to the child's age  and must be numeric;

*   The `Output` (here, `Weight`) column corresponds to the child’s weight and must be numeric.




```{r}
knitr::kable(weight)
```

Any additional column (such as height, morphology, percentage of muscle mass ...) would be treated as a covariate, and thus define multi-dimensional inputs.

Before starting to use MagmaClust, we must ensure that :

*   Our dataset contains the columns `ID`, `Input` and `Output`;

*   Their types are as previously introduced.

Therefore, we need to remove the `sex` column before using MagmaClust. 

```{r}
weight <- weight %>% select(-sex)
```

Moreover, as the children weight is not particularly affected by the gender between 0 and 5 years old, we retain all individuals in the same dataset.


## Classical pipeline

The overall process with MagmaClust can be decomposed in 3 main steps: **training, prediction** and **display of results**.
The corresponding functions are:

-   [`train_magmaclust()`](https://arthurleroy.github.io/MagmaClustR/reference/train_magmaclust.html)
-   [`pred_magmaclust()`](https://arthurleroy.github.io/MagmaClustR/reference/pred_magmaclust.html)
-   [`plot_magmaclust()`](https://arthurleroy.github.io/MagmaClustR/reference/plot_magmaclust.html)

## Apply MagmaClust on the weight database

### Data organisation

Once the data organisation step is done, we randomly select two types of children :

*   those we use to train the model;

*   the one for whom we predict future weights; let's give him the fictive name of James.

Since our dataset is quite large, we randomly select only 16 children for the training step. Even if MagmaClust performances improve with respect to the number of training children, 16 are enough to get a clear idea of how the algorithm works.


```{r}
weight_train <- weight %>% filter(ID %in% list_ID[1:20])
weight_pred <- weight %>% filter(ID == list_ID[72]) %>% filter(Input<20)
```


```{r fig.asp = 0.65, fig.width = 7, out.width = "80%"}
ggplot2::ggplot(data = weight_train,
       mapping = ggplot2::aes(x=Input,y=Output,colour=factor(ID)))+
  ggplot2::geom_point(size=1.5,alpha=0.3)+
  ggplot2::geom_point(data = weight_pred,
             size=3,
             shape=17,
             col="black") +
  ggplot2::theme_classic() +
  ggplot2::guides(colour="none")
```

The triangles correspond to James’ weights, whereas the colored dots are the children' data we use for training.

### Training

Once we split the dataset into `train` and `pred`, we can proceed to the training of the model with the `train_magmaclust()` function. Now that we have a good idea of how our data looks, we can specify values for several parameters:

*  `nb_cluster`:  as for any clustering method, we have to provide a number *K* of clusters as an hypothesis of the model. For illustration purposes, we arbitrarily set $K = 3$ in the following example. However, a dedicated model selection method based on maximising a VBIC criterion is provided in the package as [`select_nb_clusters()`](https://arthurleroy.github.io/MagmaClustR/reference/train_magma.html).

*   `kern`: the relationship between observed data and prediction targets can be control through the covariance **kernel**.
Therefore, in order to correctly fit our data, we need to choose a suitable covariance kernel.
In the case of swimmers, we want a smooth progression curve for Michaela; therefore, we specify `kern = "SE"`.

The most commonly used kernels and their properties are discussed in [the kernel cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/).
Details of available kernels and how to combine them are available in [`help train_magma`](https://arthurleroy.github.io/MagmaClustR/reference/train_magma.html). 

*   `common_hp_k`: here, we assume that the set of hyper-parameters is the same for each cluster by setting `common_hp_k = TRUE`. Thus, we suppose that all clusters share the same covariance structure. This property implies that the shapes and variations of the curves are assumed to be roughly identical from one cluster to another; the differentiation is mainly due to the mean values.

*  `common_hp_i`: as we want to share information across children, we specify `common_hp_i = TRUE`. If this assumption may appear restrictive at first glance, it actually offers a valuable way to share common patterns between tasks.

As for any GP method, initialisation of the HP may have an influence on the final optimisation and leads to inadequate prediction for pathological cases. Therefore, users may explicitly define specific initial values through the dedicated arguments: `ini_hp_k` and `ini_hp_i`.

Other parameters can also be modified; see [`help train_magmaclust`](https://arthurleroy.github.io/MagmaClustR/reference/train_magmaclust.html) for more details.



```{r}
model_clust <- train_magmaclust(data = weight_train,
                        nb_cluster = 3,
                        kern_k = "SE",
                        kern_i = "SE",
                        common_hp_k = TRUE,
                        common_hp_i = TRUE)
```


### Prediction for James

As the MagmaClust model is trained, we can predict the evolution of James' weight. To perform prediction, we need to specify two main parameters in the `pred_magmaclust()` function:

*   `data`: in our case, data associated to James;
*   `trained_model`, which corresponds to the model we just trained with the other 15 children.

We recall that we want to study weight curves during childhood. Therefore, we should predict the evolution of James’ weight between 0 and 6 years (72 months), which leads to set `grid_inputs = seq(0,72,0.1)`.



```{r}
pred_clust <- pred_magmaclust(data = weight_pred,
  trained_model = model_clust,
  grid_inputs = seq(0,72,0.1),
  plot = FALSE,
  get_hyperpost = TRUE)
```

Here we specified `get_hyperpost = TRUE` to plot the average GP of each cluster in the `plot_magmaclust()` function.
However, we do not need it if we only want the prediction of our individual.


### Plots


With the [`plot_magmaclust()`](https://arthurleroy.github.io/MagmaClustR/reference/plot_magmaclust.html) function, we can display the evolution of James' weight.  In particular, we show:

*   James' evolution curve in purple;

*   the training data points in the background. Each color corresponds to one individual;

*   the mean GP of each cluster in dotted lines thanks to the `prior_mean` parameter.


#### MagmaClust VS Magma

The MagmaClustR package also provides a prediction with the Magma model; thus, we can compare MagmaClust and Magma predictions. For more details on Magma, see [Vignette Magma](Lien). 

The graphs below correspond to James' weight prediction with MagmaClust (left) and Magma (right). 


```{r, echo = FALSE, message = FALSE, include = FALSE}
model_magma <- train_magma(data = weight_train,
                        kern_0 = "SE",
                        kern_i = "SE",
                        common_hp = FALSE)

pred_magma <- pred_magma(data=weight_pred,
                   grid_inputs = seq(0,72,0.1),
                   trained_model = model_magma,
                   plot = FALSE,
                   get_hyperpost = TRUE) 

gg2 <- plot_gp(pred_gp = pred_magma,
        data = weight_pred,
        prior_mean = pred_magma$hyperpost$mean,
        data_train = weight_train,
        size_data = 5) + 
  ggplot2::scale_y_continuous(limits = c(0,37))

data_train_with_clust = data_allocate_cluster(model_clust)

gg1 <- plot_magmaclust(pred = pred_clust,
                data = weight_pred,
                data_train = weight_train,
                prior_mean = pred_clust$hyperpost$mean,
                size_data = 5) + 
  ggplot2::scale_y_continuous(limits = c(0,37))
```


```{r, eval=FALSE}
plot_magmaclust(pred = pred_clust,
                data = weight_pred,
                prior_mean = pred_clust$hyperpost$mean,
                data_train = weight_train,
                size_data = 5)
```



```{r fig.asp = 0.7, fig.width = 10, out.width = "100%", echo = FALSE,warning=FALSE}
gg1
gg2
```


On intervals of unobserved timestamps containing data points from the training dataset ($t \in ]20, 72]$), Magma takes advantage of its multi-task component to share knowledge across individuals by estimating a relevant mean process.
However, this unique mean process appears unable to account for the group structures among children, although adequately recovering the dispersion of their weight.
Thus, MagmaClust brings a significant improvement in mean prediction thanks to its gaussian mixtures feature (through the multiple mean processes)

However, difference of prediction performances between Magma and MagmaClust depends highly on the structure of our data (*is it fuzzy or well-seperated?*), as well as on the initialisation.

We can notice oscillations in the average GPs of the clusters, which comes from several things:

  * The data itself contains oscillations;
  * We took `SE` kernels, which favor oscillations;
  * Data is observed only at particular time points.
  

## Even prettier graphics

We can distinguish 2 cases:

*   Michaela belongs to one of the clusters with a probability of 1. We can then display the associated 95% credibility interval;

*   Michaela belongs to several clusters; therefore, its evolution curve is a mixture of gaussians.
In this case, there is no associated credibility interval. However, a heatmap of probabilities is available if we want to estimate the "most likely" areas for the curve.

We can also display training individuals in the same color as their cluster mean GP by using the [`data_allocate_cluster()`](https://arthurleroy.github.io/MagmaClustR/reference/data_allocate_cluster.html) function. We also specify the arguments `data_train = data_train_with_clust` and `col_clust = TRUE`.


```{r fig.asp = 0.65, fig.width = 7, out.width = "80%"}
data_train_with_clust = data_allocate_cluster(model_clust)

plot_magmaclust(pred = pred_clust,
                cluster = "all",
                data = weight_pred,
                prior_mean = pred_clust$hyperpost$mean,
                heatmap = TRUE,
                y_grid = seq(0,37,0.1),
                data_train = data_train_with_clust,
                col_clust = TRUE,
                size_data = 5)
```

## Reference

Overall, the multi-task framework combined with clustering method provides reliable probabilistic predictions for a new child on a wide range of timestamps.
Moreover, the uncertainty provided through the predictive posterior distribution offers an adequate degree of caution to practitioners. 

For further details, the complete derivation of the algorithm and experiments are published and available in [Cluster-Specific Predictions with Multi-Task Gaussian Processes](https://arxiv.org/pdf/2011.07866.pdf).




